{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown -U\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:11:35.025389Z","iopub.execute_input":"2024-08-27T10:11:35.026384Z","iopub.status.idle":"2024-08-27T10:12:01.185072Z","shell.execute_reply.started":"2024-08-27T10:11:35.026327Z","shell.execute_reply":"2024-08-27T10:12:01.183984Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"]=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:01.187242Z","iopub.execute_input":"2024-08-27T10:12:01.187608Z","iopub.status.idle":"2024-08-27T10:12:01.192434Z","shell.execute_reply.started":"2024-08-27T10:12:01.187559Z","shell.execute_reply":"2024-08-27T10:12:01.191495Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!mkdir -p results_promise_docs_bert","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:01.193558Z","iopub.execute_input":"2024-08-27T10:12:01.193851Z","iopub.status.idle":"2024-08-27T10:12:02.225673Z","shell.execute_reply.started":"2024-08-27T10:12:01.193821Z","shell.execute_reply":"2024-08-27T10:12:02.224276Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown '1zLzqY2h10TNiq9NDKl0cJBTWSfHv014Y'","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:02.228624Z","iopub.execute_input":"2024-08-27T10:12:02.228981Z","iopub.status.idle":"2024-08-27T10:12:07.358831Z","shell.execute_reply.started":"2024-08-27T10:12:02.228946Z","shell.execute_reply":"2024-08-27T10:12:07.357638Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1zLzqY2h10TNiq9NDKl0cJBTWSfHv014Y\nTo: /kaggle/working/NFR_PROMISE.csv\n100%|██████████████████████████████████████| 81.8k/81.8k [00:00<00:00, 82.1MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Aqui empiezo el código igual al de colab","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:07.360533Z","iopub.execute_input":"2024-08-27T10:12:07.360989Z","iopub.status.idle":"2024-08-27T10:12:07.365691Z","shell.execute_reply.started":"2024-08-27T10:12:07.360941Z","shell.execute_reply":"2024-08-27T10:12:07.364736Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"FILE_REQ='/kaggle/working/NFR_PROMISE.csv'\ndata_req = load_dataset('csv', data_files=FILE_REQ, sep=';')\nprint(data_req['train']['NFR'][21])\n\ndef create_labels(row):\n  return {'labels': 'F' if row['NFR'] == 'F' else 'NF'}\n\ndata_req = data_req.map(create_labels)\ndata_req = data_req.class_encode_column('labels')\nprint(set(data_req['train']['labels']))\ndata_req = data_req['train']\ndata_req.features","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:07.366943Z","iopub.execute_input":"2024-08-27T10:12:07.367229Z","iopub.status.idle":"2024-08-27T10:12:07.625820Z","shell.execute_reply.started":"2024-08-27T10:12:07.367197Z","shell.execute_reply":"2024-08-27T10:12:07.624898Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"F\n{0, 1}\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'Num': Value(dtype='int64', id=None),\n 'Doc': Value(dtype='int64', id=None),\n 'Requisito': Value(dtype='string', id=None),\n 'NFR': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['F', 'NF'], id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n#BERT: model_name = 'google-bert/bert-base-cased'\n#DEBERTA: model_name = 'microsoft/deberta-v3-base'\n#ROBERTA: model_name = 'FacebookAI/roberta-base'\nmodel_name = 'google-bert/bert-base-cased' # modificar con el modelo que queramos cada vez","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:07.627250Z","iopub.execute_input":"2024-08-27T10:12:07.627697Z","iopub.status.idle":"2024-08-27T10:12:07.632376Z","shell.execute_reply.started":"2024-08-27T10:12:07.627652Z","shell.execute_reply":"2024-08-27T10:12:07.631352Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:07.633677Z","iopub.execute_input":"2024-08-27T10:12:07.634044Z","iopub.status.idle":"2024-08-27T10:12:09.689590Z","shell.execute_reply.started":"2024-08-27T10:12:07.634002Z","shell.execute_reply":"2024-08-27T10:12:09.688560Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588e206a54dd4763b9b1717a6f379793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40ecfbaa64914a0494102b267feb84f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dc461ff6fa4dcb86256f0f8b1f6f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7b4dab3a29461782ec5b6b9f615157"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5534b3dbd7e44333b16741e9049e9a4d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def encode_text(row, tokenizer):\n  row_encode = tokenizer(row['Requisito'], truncation=True)\n  return row_encode\n\n\nencode_data = data_req.map(encode_text, fn_kwargs={'tokenizer':tokenizer})\nencode_data\n\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\nfolds = StratifiedKFold(n_splits=10, shuffle=True)\n\nsplits = folds.split(np.zeros(data_req.num_rows), encode_data[\"labels\"])\ntest_sets = []\ntrain_val_sets = []\ni=1\nfor train_val_idxs, test_idxs in splits:\n  print (i)\n  print(test_idxs)\n  print('*******')\n  data_test = encode_data.select(test_idxs)\n  data_train = encode_data.select(train_val_idxs)\n  data_train_val = data_train.train_test_split(train_size=0.9, stratify_by_column='labels')\n  #data_train_inner, data_val_inner = data_train.train_test_split(train_size=0.9)\n  test_sets.append(data_test)\n  train_val_sets.append(data_train_val)\n  i = i + 1\n\nprint(test_sets[0])\nprint(train_val_sets[0]['test'])","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:09.691005Z","iopub.execute_input":"2024-08-27T10:12:09.691371Z","iopub.status.idle":"2024-08-27T10:12:10.169701Z","shell.execute_reply.started":"2024-08-27T10:12:09.691335Z","shell.execute_reply":"2024-08-27T10:12:10.168585Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d1293623314e5786f6d51e1bbccf8b"}},"metadata":{}},{"name":"stdout","text":"1\n[  1   2  22  36  40  50  68  73  79  88 117 118 125 136 142 157 160 175\n 177 181 199 216 220 226 227 238 243 248 268 274 277 298 322 323 325 333\n 358 360 369 384 389 397 414 415 416 440 449 456 465 472 483 486 492 502\n 504 509 538 540 568 572 578 600 618]\n*******\n2\n[  4  31  33  42  52  56  62  63  67  78  80  94  96 107 111 124 127 129\n 140 158 169 182 187 235 254 258 263 264 290 297 299 302 321 336 338 344\n 364 368 406 408 412 426 434 470 471 475 479 499 503 513 523 527 542 544\n 561 567 569 575 589 590 596 597 620]\n*******\n3\n[  6   8   9  14  39  51  75  77  87  90  93 100 131 141 143 144 148 154\n 155 167 178 194 196 218 222 240 242 265 267 279 280 289 292 295 310 348\n 359 395 398 400 410 429 435 443 450 468 477 488 489 508 510 516 520 528\n 539 566 570 574 580 612 613 621 624]\n*******\n4\n[  7  11  13  17  19  29  45  55  76  82  91 114 126 161 162 174 191 198\n 207 224 232 236 252 253 259 272 275 287 293 300 301 307 308 320 337 339\n 349 353 357 363 367 379 386 390 394 403 418 424 433 452 478 480 485 498\n 500 501 506 524 529 530 584 610 616]\n*******\n5\n[ 15  25  32  34  41  69  70 102 103 106 110 121 128 166 170 183 214 219\n 228 229 230 234 237 244 273 281 303 306 312 314 354 365 366 370 372 377\n 380 404 413 417 421 431 448 451 458 462 463 464 467 481 482 487 496 518\n 533 543 560 582 591 595 602 609 611]\n*******\n6\n[ 12  16  47  48  61  74  84  95  99 109 130 133 138 149 153 180 186 189\n 203 215 221 257 260 269 288 309 326 329 347 351 381 382 387 388 411 423\n 432 437 446 447 459 460 466 469 490 517 522 531 535 536 547 551 552 559\n 562 563 571 592 594 604 615 619]\n*******\n7\n[  5  26  30  37  43  44  46  60  65  66  71  85  97 116 122 137 145 172\n 173 176 179 200 202 206 209 249 262 276 278 283 324 327 328 330 331 332\n 340 350 355 356 378 401 420 422 425 427 438 441 444 461 476 494 511 512\n 541 550 555 577 588 598 599 607]\n*******\n8\n[ 24  35  54  83 101 115 119 139 146 147 151 159 165 185 190 195 213 223\n 239 246 255 261 271 282 294 305 315 317 318 319 334 346 361 362 375 385\n 391 392 409 428 430 436 453 473 484 493 495 507 519 521 525 534 548 553\n 558 565 573 576 581 603 605 622]\n*******\n9\n[  0   3  10  20  27  28  57  72  92  98 105 108 120 123 134 135 150 163\n 164 171 192 193 197 204 211 231 241 245 247 250 251 284 286 296 311 335\n 341 342 345 352 393 396 402 405 419 439 442 445 454 491 497 515 532 546\n 549 564 579 585 587 601 608 614]\n*******\n10\n[ 18  21  23  38  49  53  58  59  64  81  86  89 104 112 113 132 152 156\n 168 184 188 201 205 208 210 212 217 225 233 256 266 270 285 291 304 313\n 316 343 371 373 374 376 383 399 407 455 457 474 505 514 526 537 545 554\n 556 557 583 586 593 606 617 623]\n*******\nDataset({\n    features: ['Num', 'Doc', 'Requisito', 'NFR', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 63\n})\nDataset({\n    features: ['Num', 'Doc', 'Requisito', 'NFR', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 57\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:10.173670Z","iopub.execute_input":"2024-08-27T10:12:10.174283Z","iopub.status.idle":"2024-08-27T10:12:12.131889Z","shell.execute_reply.started":"2024-08-27T10:12:10.174239Z","shell.execute_reply":"2024-08-27T10:12:12.131026Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd7a25d063a4dc0825e1a5f7f2b1a8c"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:12.133220Z","iopub.execute_input":"2024-08-27T10:12:12.133963Z","iopub.status.idle":"2024-08-27T10:12:12.138360Z","shell.execute_reply.started":"2024-08-27T10:12:12.133916Z","shell.execute_reply":"2024-08-27T10:12:12.137438Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis = 1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# load metric\nmetric_name = 'f1'\nmetric = evaluate.load(metric_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:12.139603Z","iopub.execute_input":"2024-08-27T10:12:12.139960Z","iopub.status.idle":"2024-08-27T10:12:12.711406Z","shell.execute_reply.started":"2024-08-27T10:12:12.139927Z","shell.execute_reply":"2024-08-27T10:12:12.710620Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Si uso dos GPUs poner batch size 16, porque van 16 a cada una = 32","metadata":{}},{"cell_type":"code","source":"import glob\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import classification_report\n\nlr_values =[2e-5, 5e-5]\n\nfor j in range(len(train_val_sets)):\n    best_lr = -1\n    best_f1 = -1\n\n    # para escribir los resultados\n    with open(f'results_promise_docs_bert/res_bert_{j}.txt', 'w') as f:\n    # NOTA: si no va, deshacer el tab extra del siguiente bucle for\n      for i, lr in enumerate(lr_values):\n        print(\"================================================\")\n        print(\"j = \" + str(j) + \" // lr = \" + str(lr))\n        print(\"================================================\")\n      # argumentos para el entrenamiento\n        training_args = TrainingArguments(\n            #output_dir=\"my_checkpoint\"+str(j),\n            output_dir=f\"my_checkpoint_{i}\",\n            overwrite_output_dir = True,\n            num_train_epochs=10,\n            #para probar tanto promise como nuestro coger 10 epochs\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            save_total_limit = 1,\n            load_best_model_at_end=True,\n            logging_strategy='epoch',\n            optim=\"adamw_torch\",\n            per_device_train_batch_size=32,\n            #lo dejaremos en 32: per_device_eval_batch_size=32,\n            learning_rate=lr,\n            #probar tanto 2e-5 como 5e-5\n            weight_decay=0.01,\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n        # declarar el \"entrenador\"\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_val_sets[j]['train'],\n            eval_dataset=train_val_sets[j]['test'],\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics\n        )\n\n        # realizar el entrenamiento\n        trainer.train()\n\n        out_pred = trainer.predict(train_val_sets[j]['test'])\n        if out_pred.metrics['test_f1'] > best_f1:\n          best_f1 = out_pred.metrics['test_f1']\n          best_lr = i\n      # cargar el mejor modelo y resultados para test\n      best_dir = glob.glob(f\"my_checkpoint_{best_lr}/checkpoint*\")\n      print(best_dir)\n      model_best = AutoModelForSequenceClassification.from_pretrained(best_dir[0])\n      trainer_best = Trainer(\n            model=model_best,\n            args=None,\n            train_dataset=None,\n            eval_dataset=None,\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics\n        )\n\n      test_data = test_sets[j]\n      outputs_pred = trainer_best.predict(test_dataset=test_data)\n\n      # model predictions\n      predictions = np.argmax(outputs_pred.predictions, axis=1)\n\n      #d_res = classification_report(test_data['labels'], predictions, digits=3, return_dict = True)\n      d_res = classification_report(test_data['labels'], predictions, digits=3, output_dict=True)\n      print(d_res)\n      f.write(\"================== Best lr ================\\n\")\n      f.write(str(best_lr))\n      f.write(\"\\n\")\n      f.write( \"================== Resultados ================\\n\")\n      f.write(str(d_res))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:12:12.712576Z","iopub.execute_input":"2024-08-27T10:12:12.712889Z","iopub.status.idle":"2024-08-27T10:38:38.684695Z","shell.execute_reply.started":"2024-08-27T10:12:12.712855Z","shell.execute_reply":"2024-08-27T10:38:38.683686Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 0 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:12, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.675500</td>\n      <td>0.653079</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.564800</td>\n      <td>0.285978</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.256500</td>\n      <td>0.146408</td>\n      <td>0.953846</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.157900</td>\n      <td>0.138448</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.109200</td>\n      <td>0.182466</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.066400</td>\n      <td>0.119527</td>\n      <td>0.955224</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.058900</td>\n      <td>0.144745</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.033800</td>\n      <td>0.141236</td>\n      <td>0.955224</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.027100</td>\n      <td>0.173096</td>\n      <td>0.955224</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.028200</td>\n      <td>0.178862</td>\n      <td>0.955224</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 0 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:18, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.649800</td>\n      <td>0.507619</td>\n      <td>0.790698</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.316100</td>\n      <td>0.330035</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.135600</td>\n      <td>0.350793</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.127200</td>\n      <td>0.345632</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.067400</td>\n      <td>0.351330</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.033800</td>\n      <td>0.416162</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.044400</td>\n      <td>0.488257</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.021700</td>\n      <td>0.453893</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.014700</td>\n      <td>0.534211</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.007200</td>\n      <td>0.600851</td>\n      <td>0.929577</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-96']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.92, 'recall': 0.8846153846153846, 'f1-score': 0.9019607843137256, 'support': 26}, '1': {'precision': 0.9210526315789473, 'recall': 0.9459459459459459, 'f1-score': 0.9333333333333332, 'support': 37}, 'accuracy': 0.9206349206349206, 'macro avg': {'precision': 0.9205263157894736, 'recall': 0.9152806652806653, 'f1-score': 0.9176470588235295, 'support': 63}, 'weighted avg': {'precision': 0.9206182121971597, 'recall': 0.9206349206349206, 'f1-score': 0.920385932150638, 'support': 63}}\n================================================\nj = 1 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:13, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.683600</td>\n      <td>0.650686</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.592700</td>\n      <td>0.495522</td>\n      <td>0.790698</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.396700</td>\n      <td>0.327960</td>\n      <td>0.918919</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.209000</td>\n      <td>0.244783</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.129400</td>\n      <td>0.219444</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.089600</td>\n      <td>0.220219</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.074700</td>\n      <td>0.254856</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.041400</td>\n      <td>0.457579</td>\n      <td>0.918919</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.038600</td>\n      <td>0.272094</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.030400</td>\n      <td>0.338415</td>\n      <td>0.944444</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 1 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.640000</td>\n      <td>0.472985</td>\n      <td>0.876712</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.352700</td>\n      <td>0.418105</td>\n      <td>0.894737</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.187900</td>\n      <td>0.401469</td>\n      <td>0.869565</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.115300</td>\n      <td>0.620247</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.068900</td>\n      <td>0.646409</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.039600</td>\n      <td>0.645124</td>\n      <td>0.918919</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.035700</td>\n      <td>0.691232</td>\n      <td>0.918919</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.016200</td>\n      <td>0.715127</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.014700</td>\n      <td>0.733214</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.009300</td>\n      <td>0.739185</td>\n      <td>0.918919</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-80']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.7878787878787878, 'recall': 1.0, 'f1-score': 0.8813559322033898, 'support': 26}, '1': {'precision': 1.0, 'recall': 0.8108108108108109, 'f1-score': 0.8955223880597014, 'support': 37}, 'accuracy': 0.8888888888888888, 'macro avg': {'precision': 0.8939393939393939, 'recall': 0.9054054054054055, 'f1-score': 0.8884391601315456, 'support': 63}, 'weighted avg': {'precision': 0.9124579124579124, 'recall': 0.8888888888888888, 'f1-score': 0.8896759142142394, 'support': 63}}\n================================================\nj = 2 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:13, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.673000</td>\n      <td>0.641950</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.541600</td>\n      <td>0.389205</td>\n      <td>0.865672</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.306900</td>\n      <td>0.301699</td>\n      <td>0.901408</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.166300</td>\n      <td>0.266896</td>\n      <td>0.885714</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.096100</td>\n      <td>0.291833</td>\n      <td>0.911765</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.062500</td>\n      <td>0.275272</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.067800</td>\n      <td>0.458058</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.049300</td>\n      <td>0.457012</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.037200</td>\n      <td>0.472975</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.027900</td>\n      <td>0.504569</td>\n      <td>0.929577</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 2 // lr = 5e-05\n================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:25, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.667900</td>\n      <td>0.496633</td>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.326900</td>\n      <td>0.308095</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.218100</td>\n      <td>0.785360</td>\n      <td>0.871795</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.138400</td>\n      <td>0.386286</td>\n      <td>0.891892</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.068700</td>\n      <td>0.492374</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.042100</td>\n      <td>0.467083</td>\n      <td>0.911765</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.051800</td>\n      <td>0.617668</td>\n      <td>0.891892</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.016100</td>\n      <td>0.440175</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.018300</td>\n      <td>0.397878</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003800</td>\n      <td>0.420424</td>\n      <td>0.941176</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-32']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.8888888888888888, 'recall': 0.9230769230769231, 'f1-score': 0.9056603773584906, 'support': 26}, '1': {'precision': 0.9444444444444444, 'recall': 0.918918918918919, 'f1-score': 0.9315068493150684, 'support': 37}, 'accuracy': 0.9206349206349206, 'macro avg': {'precision': 0.9166666666666666, 'recall': 0.9209979209979211, 'f1-score': 0.9185836133367795, 'support': 63}, 'weighted avg': {'precision': 0.9215167548500882, 'recall': 0.9206349206349206, 'f1-score': 0.9208400513647348, 'support': 63}}\n================================================\nj = 3 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.662600</td>\n      <td>0.628961</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.544300</td>\n      <td>0.365005</td>\n      <td>0.885714</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.312300</td>\n      <td>0.294885</td>\n      <td>0.931507</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.229300</td>\n      <td>0.160131</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.131200</td>\n      <td>0.189051</td>\n      <td>0.971429</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.088000</td>\n      <td>0.169150</td>\n      <td>0.971429</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.057100</td>\n      <td>0.168151</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.053600</td>\n      <td>0.231654</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.025000</td>\n      <td>0.224524</td>\n      <td>0.971429</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.030300</td>\n      <td>0.224878</td>\n      <td>0.971429</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 3 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:14, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.630700</td>\n      <td>0.431613</td>\n      <td>0.846154</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.322000</td>\n      <td>0.250133</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.223800</td>\n      <td>0.569257</td>\n      <td>0.819277</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.147200</td>\n      <td>0.152575</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.083800</td>\n      <td>0.163628</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.039900</td>\n      <td>0.314497</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.038800</td>\n      <td>0.384693</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.018300</td>\n      <td>0.279423</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.008800</td>\n      <td>0.378670</td>\n      <td>0.942857</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003500</td>\n      <td>0.388876</td>\n      <td>0.942857</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-64']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.9259259259259259, 'recall': 0.9615384615384616, 'f1-score': 0.9433962264150944, 'support': 26}, '1': {'precision': 0.9722222222222222, 'recall': 0.9459459459459459, 'f1-score': 0.9589041095890412, 'support': 37}, 'accuracy': 0.9523809523809523, 'macro avg': {'precision': 0.9490740740740741, 'recall': 0.9537422037422038, 'f1-score': 0.9511501680020678, 'support': 63}, 'weighted avg': {'precision': 0.9531158142269253, 'recall': 0.9523809523809523, 'f1-score': 0.9525040308188408, 'support': 63}}\n================================================\nj = 4 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:19, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.677100</td>\n      <td>0.643205</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.540300</td>\n      <td>0.513454</td>\n      <td>0.864865</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.328500</td>\n      <td>0.382023</td>\n      <td>0.865672</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.166600</td>\n      <td>0.417210</td>\n      <td>0.865672</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.107100</td>\n      <td>0.480600</td>\n      <td>0.892308</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.080300</td>\n      <td>0.445455</td>\n      <td>0.878788</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.035500</td>\n      <td>0.497959</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.023400</td>\n      <td>0.524952</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.015400</td>\n      <td>0.526153</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.013600</td>\n      <td>0.530823</td>\n      <td>0.914286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 4 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:22, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.667700</td>\n      <td>0.553385</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.368800</td>\n      <td>0.462577</td>\n      <td>0.843750</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.204200</td>\n      <td>0.394859</td>\n      <td>0.878788</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.096000</td>\n      <td>0.801741</td>\n      <td>0.891892</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.060100</td>\n      <td>0.880542</td>\n      <td>0.891892</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.052200</td>\n      <td>0.520550</td>\n      <td>0.942857</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.017400</td>\n      <td>0.715787</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.007400</td>\n      <td>0.592652</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.005300</td>\n      <td>0.604417</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.002700</td>\n      <td>0.581200</td>\n      <td>0.942857</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-48']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.88, 'recall': 0.8461538461538461, 'f1-score': 0.8627450980392156, 'support': 26}, '1': {'precision': 0.8947368421052632, 'recall': 0.918918918918919, 'f1-score': 0.9066666666666667, 'support': 37}, 'accuracy': 0.8888888888888888, 'macro avg': {'precision': 0.8873684210526316, 'recall': 0.8825363825363826, 'f1-score': 0.8847058823529412, 'support': 63}, 'weighted avg': {'precision': 0.8886549707602339, 'recall': 0.8888888888888888, 'f1-score': 0.8885403050108932, 'support': 63}}\n================================================\nj = 5 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:21, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.669400</td>\n      <td>0.607830</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.564400</td>\n      <td>0.392997</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.352300</td>\n      <td>0.234608</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.200400</td>\n      <td>0.148823</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.132700</td>\n      <td>0.144195</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.099400</td>\n      <td>0.184439</td>\n      <td>0.953846</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.059200</td>\n      <td>0.206695</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.037000</td>\n      <td>0.203725</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.026300</td>\n      <td>0.192011</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.023200</td>\n      <td>0.189572</td>\n      <td>0.970588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 5 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:14, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.654700</td>\n      <td>0.508927</td>\n      <td>0.865672</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.413900</td>\n      <td>0.276373</td>\n      <td>0.876712</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.208400</td>\n      <td>0.186966</td>\n      <td>0.939394</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.122900</td>\n      <td>0.360181</td>\n      <td>0.904110</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.147700</td>\n      <td>0.178368</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.051400</td>\n      <td>0.202624</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.020800</td>\n      <td>0.313317</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.017200</td>\n      <td>0.235148</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.008400</td>\n      <td>0.220700</td>\n      <td>0.970588</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.006600</td>\n      <td>0.222002</td>\n      <td>0.970588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-80']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.9545454545454546, 'recall': 0.84, 'f1-score': 0.8936170212765958, 'support': 25}, '1': {'precision': 0.9, 'recall': 0.972972972972973, 'f1-score': 0.935064935064935, 'support': 37}, 'accuracy': 0.9193548387096774, 'macro avg': {'precision': 0.9272727272727272, 'recall': 0.9064864864864866, 'f1-score': 0.9143409781707654, 'support': 62}, 'weighted avg': {'precision': 0.9219941348973608, 'recall': 0.9193548387096774, 'f1-score': 0.9183520666018951, 'support': 62}}\n================================================\nj = 6 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.677000</td>\n      <td>0.621279</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.543400</td>\n      <td>0.390277</td>\n      <td>0.853333</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.333800</td>\n      <td>0.402161</td>\n      <td>0.864865</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.260300</td>\n      <td>0.396420</td>\n      <td>0.876712</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.146900</td>\n      <td>0.439610</td>\n      <td>0.857143</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.092700</td>\n      <td>0.557344</td>\n      <td>0.857143</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.048400</td>\n      <td>0.648717</td>\n      <td>0.869565</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.044100</td>\n      <td>0.546363</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.027500</td>\n      <td>0.702010</td>\n      <td>0.882353</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.026700</td>\n      <td>0.703093</td>\n      <td>0.882353</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 6 // lr = 5e-05\n================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.640000</td>\n      <td>0.406552</td>\n      <td>0.852459</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.317000</td>\n      <td>0.212998</td>\n      <td>0.953846</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.164000</td>\n      <td>0.349536</td>\n      <td>0.925373</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.074700</td>\n      <td>0.474961</td>\n      <td>0.898551</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.044400</td>\n      <td>0.471486</td>\n      <td>0.909091</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.030500</td>\n      <td>0.499990</td>\n      <td>0.901408</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.016400</td>\n      <td>0.541412</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.001100</td>\n      <td>0.529184</td>\n      <td>0.895522</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000700</td>\n      <td>0.541104</td>\n      <td>0.895522</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000700</td>\n      <td>0.548376</td>\n      <td>0.909091</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-32']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.8214285714285714, 'recall': 0.92, 'f1-score': 0.8679245283018867, 'support': 25}, '1': {'precision': 0.9411764705882353, 'recall': 0.8648648648648649, 'f1-score': 0.9014084507042254, 'support': 37}, 'accuracy': 0.8870967741935484, 'macro avg': {'precision': 0.8813025210084033, 'recall': 0.8924324324324324, 'f1-score': 0.884666489503056, 'support': 62}, 'weighted avg': {'precision': 0.8928910273786933, 'recall': 0.8870967741935484, 'f1-score': 0.8879068690903791, 'support': 62}}\n================================================\nj = 7 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:20, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.681100</td>\n      <td>0.636413</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.574200</td>\n      <td>0.495704</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.349600</td>\n      <td>0.358292</td>\n      <td>0.929577</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.169500</td>\n      <td>0.439733</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.097200</td>\n      <td>0.449383</td>\n      <td>0.911765</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.085100</td>\n      <td>0.449119</td>\n      <td>0.911765</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.044400</td>\n      <td>0.572011</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.042100</td>\n      <td>0.514325</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.033800</td>\n      <td>0.614883</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.022000</td>\n      <td>0.575815</td>\n      <td>0.914286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 7 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:15, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.613400</td>\n      <td>0.430270</td>\n      <td>0.830769</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.378800</td>\n      <td>0.323605</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.198800</td>\n      <td>0.357976</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.110300</td>\n      <td>0.544874</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.068600</td>\n      <td>0.588205</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.047800</td>\n      <td>0.631962</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.025700</td>\n      <td>0.463408</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.032700</td>\n      <td>0.636173</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.017400</td>\n      <td>0.661570</td>\n      <td>0.914286</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.010100</td>\n      <td>0.712877</td>\n      <td>0.914286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-48']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.8846153846153846, 'recall': 0.92, 'f1-score': 0.9019607843137256, 'support': 25}, '1': {'precision': 0.9444444444444444, 'recall': 0.918918918918919, 'f1-score': 0.9315068493150684, 'support': 37}, 'accuracy': 0.9193548387096774, 'macro avg': {'precision': 0.9145299145299145, 'recall': 0.9194594594594595, 'f1-score': 0.916733816814397, 'support': 62}, 'weighted avg': {'precision': 0.9203198235456299, 'recall': 0.9193548387096774, 'f1-score': 0.9195931134274302, 'support': 62}}\n================================================\nj = 8 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.686800</td>\n      <td>0.665333</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.619600</td>\n      <td>0.467795</td>\n      <td>0.846154</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.307400</td>\n      <td>0.319061</td>\n      <td>0.885246</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.149000</td>\n      <td>0.296077</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.086200</td>\n      <td>0.340875</td>\n      <td>0.925373</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.061300</td>\n      <td>0.288017</td>\n      <td>0.953846</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.042900</td>\n      <td>0.363357</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.026200</td>\n      <td>0.441543</td>\n      <td>0.911765</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.041900</td>\n      <td>0.340461</td>\n      <td>0.953846</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.019100</td>\n      <td>0.347750</td>\n      <td>0.953846</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 8 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:19, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.662000</td>\n      <td>0.489459</td>\n      <td>0.813559</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.397800</td>\n      <td>0.466660</td>\n      <td>0.866667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.188400</td>\n      <td>0.333477</td>\n      <td>0.920635</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.093600</td>\n      <td>0.428681</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.025000</td>\n      <td>0.523073</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.055400</td>\n      <td>0.557926</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.028300</td>\n      <td>0.541968</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.016000</td>\n      <td>0.481407</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.003600</td>\n      <td>0.519501</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003200</td>\n      <td>0.543537</td>\n      <td>0.923077</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-96']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.8461538461538461, 'recall': 0.88, 'f1-score': 0.8627450980392156, 'support': 25}, '1': {'precision': 0.9166666666666666, 'recall': 0.8918918918918919, 'f1-score': 0.9041095890410958, 'support': 37}, 'accuracy': 0.8870967741935484, 'macro avg': {'precision': 0.8814102564102564, 'recall': 0.885945945945946, 'f1-score': 0.8834273435401557, 'support': 62}, 'weighted avg': {'precision': 0.8882340777502067, 'recall': 0.8870967741935484, 'f1-score': 0.8874303587984022, 'support': 62}}\n================================================\nj = 9 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:18, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.686900</td>\n      <td>0.644087</td>\n      <td>0.747253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.581600</td>\n      <td>0.487351</td>\n      <td>0.853333</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.373700</td>\n      <td>0.301931</td>\n      <td>0.892308</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.207500</td>\n      <td>0.247033</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.142200</td>\n      <td>0.239212</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.089800</td>\n      <td>0.282680</td>\n      <td>0.927536</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.060200</td>\n      <td>0.329680</td>\n      <td>0.925373</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.039700</td>\n      <td>0.317463</td>\n      <td>0.898551</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.046100</td>\n      <td>0.340012</td>\n      <td>0.925373</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.030900</td>\n      <td>0.350007</td>\n      <td>0.925373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 9 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 01:18, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.617200</td>\n      <td>0.463583</td>\n      <td>0.842105</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.339900</td>\n      <td>0.241925</td>\n      <td>0.925373</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.169800</td>\n      <td>0.127484</td>\n      <td>0.971429</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.100000</td>\n      <td>0.251119</td>\n      <td>0.923077</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.047700</td>\n      <td>0.122435</td>\n      <td>0.955224</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.047200</td>\n      <td>0.242869</td>\n      <td>0.957746</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.016600</td>\n      <td>0.248334</td>\n      <td>0.956522</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.007900</td>\n      <td>0.260686</td>\n      <td>0.971429</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.008400</td>\n      <td>0.380050</td>\n      <td>0.941176</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003600</td>\n      <td>0.387468</td>\n      <td>0.941176</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-80']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'0': {'precision': 0.9583333333333334, 'recall': 0.92, 'f1-score': 0.9387755102040817, 'support': 25}, '1': {'precision': 0.9473684210526315, 'recall': 0.972972972972973, 'f1-score': 0.9599999999999999, 'support': 37}, 'accuracy': 0.9516129032258065, 'macro avg': {'precision': 0.9528508771929824, 'recall': 0.9464864864864866, 'f1-score': 0.9493877551020408, 'support': 62}, 'weighted avg': {'precision': 0.9517897566496888, 'recall': 0.9516129032258065, 'f1-score': 0.9514417379855167, 'support': 62}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Aquí acaba el código de colab. \nFalta hacer el zip para poder descargar los resultados (y opcionalmente usar el bot para que me avise del final de la ejecución)","metadata":{}},{"cell_type":"code","source":"import requests\n\n# Definir la URL base y los parámetros\nurl = \"https://api.callmebot.com/whatsapp.php\"\nparams = {\n    \"phone\": \"+34653508040\",\n    \"apikey\": \"1512070\",\n    \"text\": \"🤖¡Ejecución terminada! 🤖\\nVe a echarle un vistazo ☝🤓\"\n}\n\n# Enviar la solicitud GET\nresponse = requests.get(url, params=params)\n\n# Imprimir el estado de la respuesta\nprint(response.status_code)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:38:38.685945Z","iopub.execute_input":"2024-08-27T10:38:38.686260Z","iopub.status.idle":"2024-08-27T10:38:39.266657Z","shell.execute_reply.started":"2024-08-27T10:38:38.686225Z","shell.execute_reply":"2024-08-27T10:38:39.265675Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"200\n<p>Message to: +34653508040<p>Text to send: 🤖¡Ejecución terminada! 🤖%0AVe a echarle un vistazo ☝🤓<p><b>Message queued.</b> You will receive it in a few seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r results_promise_docs_bert.zip results_promise_docs_bert\nfrom IPython.display import FileLink \nFileLink(r'results_promise_docs_bert.zip')","metadata":{"execution":{"iopub.status.busy":"2024-08-27T10:38:39.268425Z","iopub.execute_input":"2024-08-27T10:38:39.268870Z","iopub.status.idle":"2024-08-27T10:38:44.722937Z","shell.execute_reply.started":"2024-08-27T10:38:39.268822Z","shell.execute_reply":"2024-08-27T10:38:44.721656Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: results_promise_stratified_roberta/ (stored 0%)\n  adding: results_promise_stratified_roberta/res_roberta_7.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_8.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_0.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_9.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_3.txt (deflated 61%)\n  adding: results_promise_stratified_roberta/res_roberta_1.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_2.txt (deflated 62%)\n  adding: results_promise_stratified_roberta/res_roberta_6.txt (deflated 58%)\n  adding: results_promise_stratified_roberta/res_roberta_4.txt (deflated 60%)\n  adding: results_promise_stratified_roberta/res_roberta_5.txt (deflated 60%)\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/results_promise_stratified_roberta.zip","text/html":"<a href='results_promise_stratified_roberta.zip' target='_blank'>results_promise_stratified_roberta.zip</a><br>"},"metadata":{}}]}]}