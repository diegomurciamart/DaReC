{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown -U\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:22.638508Z","iopub.execute_input":"2024-08-26T15:45:22.639054Z","iopub.status.idle":"2024-08-26T15:45:50.529182Z","shell.execute_reply.started":"2024-08-26T15:45:22.638992Z","shell.execute_reply":"2024-08-26T15:45:50.528179Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"]=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:50.531041Z","iopub.execute_input":"2024-08-26T15:45:50.531426Z","iopub.status.idle":"2024-08-26T15:45:50.536383Z","shell.execute_reply.started":"2024-08-26T15:45:50.531389Z","shell.execute_reply":"2024-08-26T15:45:50.535147Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!mkdir -p results_temas_bert","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:50.537619Z","iopub.execute_input":"2024-08-26T15:45:50.537998Z","iopub.status.idle":"2024-08-26T15:45:51.527503Z","shell.execute_reply.started":"2024-08-26T15:45:50.537964Z","shell.execute_reply":"2024-08-26T15:45:51.526278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!gdown '1pl9hGX3_4lMJvl4thbwkEXRCTi_dH1au'","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:51.530056Z","iopub.execute_input":"2024-08-26T15:45:51.530423Z","iopub.status.idle":"2024-08-26T15:45:56.656782Z","shell.execute_reply.started":"2024-08-26T15:45:51.530386Z","shell.execute_reply":"2024-08-26T15:45:56.655795Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1EjMX7Y6H5adIE2VqgnYuD6kzNOkmeUu3\nTo: /kaggle/working/my_Dataset_29JUL.csv\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 437k/437k [00:00<00:00, 108MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Aqui empiezo el cÃ³digo igual al de colab","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:56.658203Z","iopub.execute_input":"2024-08-26T15:45:56.658534Z","iopub.status.idle":"2024-08-26T15:45:58.391509Z","shell.execute_reply.started":"2024-08-26T15:45:56.658499Z","shell.execute_reply":"2024-08-26T15:45:58.390569Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"FILE_REQ='/kaggle/working/ReWoRC_Dataset_req.csv'\ndata_req = load_dataset('csv', data_files=FILE_REQ, sep=';')\nprint(data_req['train']['NFR'][21])\n\ndef create_labels(row):\n  return {'id_doc': (\n            'INT' if 'INT' in row['Referencia'] else\n            'JUE' if 'JUE' in row['Referencia'] else\n            'MON' if 'MON' in row['Referencia'] else\n            'CON' if 'CON' in row['Referencia'] else\n            'GES' if 'GES' in row['Referencia'] else\n            'OTR' if 'OTR' in row['Referencia'] else\n            'error'\n        )}\ndata_req=data_req.map(lambda r: {'labels': r['NFR']})\n\ndata_req = data_req.map(create_labels)\ndata_req = data_req.class_encode_column('labels')\ndata_req = data_req.class_encode_column('id_doc')\ndata_req = data_req['train']\ndata_req.features","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:58.392832Z","iopub.execute_input":"2024-08-26T15:45:58.393758Z","iopub.status.idle":"2024-08-26T15:45:58.824836Z","shell.execute_reply.started":"2024-08-26T15:45:58.393707Z","shell.execute_reply":"2024-08-26T15:45:58.823905Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"No funcional\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'Num': Value(dtype='int64', id=None),\n 'Referencia': Value(dtype='string', id=None),\n 'Requisito': Value(dtype='string', id=None),\n 'Documento': Value(dtype='string', id=None),\n 'NFR': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['Funcional', 'No funcional'], id=None),\n 'id_doc': ClassLabel(names=['CON', 'GES', 'INT', 'JUE', 'MON', 'OTR'], id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n#BERT: model_name = 'google-bert/bert-base-cased'\n#DEBERTA: model_name = 'microsoft/deberta-v3-base'\n#ROBERTA: model_name = 'FacebookAI/roberta-base'\nmodel_name = 'google-bert/bert-base-cased' # modificar con el modelo que queramos cada vez","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:45:58.826114Z","iopub.execute_input":"2024-08-26T15:45:58.826567Z","iopub.status.idle":"2024-08-26T15:46:01.704219Z","shell.execute_reply.started":"2024-08-26T15:45:58.826532Z","shell.execute_reply":"2024-08-26T15:46:01.703387Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:01.705398Z","iopub.execute_input":"2024-08-26T15:46:01.705711Z","iopub.status.idle":"2024-08-26T15:46:01.989832Z","shell.execute_reply.started":"2024-08-26T15:46:01.705677Z","shell.execute_reply":"2024-08-26T15:46:01.988787Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def encode_text(row, tokenizer):\n  row_encode = tokenizer(row['Requisito'], truncation=True)\n  return row_encode\n\n\nencode_data = data_req.map(encode_text, fn_kwargs={'tokenizer':tokenizer})\nencode_data\n\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nfolds = KFold(n_splits=6, shuffle=True)\n\nsplits = folds.split(np.zeros(6))\ntest_sets = []\ntrain_val_sets = []\ni=1\nfor train_val_idxs, test_idxs in splits:\n  print (i)\n  print(test_idxs)\n  print('*******')\n  data_test = encode_data.filter(lambda r : r['id_doc'] in test_idxs)\n  data_train = encode_data.filter(lambda r : r['id_doc'] in train_val_idxs)\n  data_train_val = data_train.train_test_split(train_size=0.9, stratify_by_column='labels')\n  print(data_test)\n  print(data_train)\n  print(data_test.num_rows + data_train.num_rows)\n  print(encode_data.num_rows == (data_test.num_rows + data_train.num_rows))\n  print(\"***\")\n\n\n  test_sets.append(data_test)\n  train_val_sets.append(data_train_val)\n  i = i + 1","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:01.991379Z","iopub.execute_input":"2024-08-26T15:46:01.991806Z","iopub.status.idle":"2024-08-26T15:46:02.768711Z","shell.execute_reply.started":"2024-08-26T15:46:01.991758Z","shell.execute_reply":"2024-08-26T15:46:02.767802Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"1\n[5]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 893\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 1498\n})\n2391\nTrue\n***\n2\n[2]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 173\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 2218\n})\n2391\nTrue\n***\n3\n[0]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 110\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 2281\n})\n2391\nTrue\n***\n4\n[3]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 170\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 2221\n})\n2391\nTrue\n***\n5\n[1]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 726\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 1665\n})\n2391\nTrue\n***\n6\n[4]\n*******\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 319\n})\nDataset({\n    features: ['Num', 'Referencia', 'Requisito', 'Documento', 'NFR', 'labels', 'id_doc', 'input_ids', 'attention_mask'],\n    num_rows: 2072\n})\n2391\nTrue\n***\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:02.771925Z","iopub.execute_input":"2024-08-26T15:46:02.772427Z","iopub.status.idle":"2024-08-26T15:46:03.775950Z","shell.execute_reply.started":"2024-08-26T15:46:02.772389Z","shell.execute_reply":"2024-08-26T15:46:03.774952Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:03.777619Z","iopub.execute_input":"2024-08-26T15:46:03.778139Z","iopub.status.idle":"2024-08-26T15:46:09.138570Z","shell.execute_reply.started":"2024-08-26T15:46:03.778104Z","shell.execute_reply":"2024-08-26T15:46:09.137645Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis = 1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# load metric\nmetric_name = 'f1'\nmetric = evaluate.load(metric_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:09.139608Z","iopub.execute_input":"2024-08-26T15:46:09.140147Z","iopub.status.idle":"2024-08-26T15:46:09.626317Z","shell.execute_reply.started":"2024-08-26T15:46:09.140113Z","shell.execute_reply":"2024-08-26T15:46:09.625498Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import glob\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import classification_report\n\nlr_values =[2e-5, 5e-5]\n\nfor j in range(len(train_val_sets)):\n    best_lr = -1\n    best_f1 = -1\n\n    # para escribir los resultados\n    with open(f'results_temas_bert/res_temas_bert_{j}.txt', 'w') as f:\n    # NOTA: si no va, deshacer el tab extra del siguiente bucle for\n      for i, lr in enumerate(lr_values):\n        print(\"================================================\")\n        print(\"j = \" + str(j) + \" // lr = \" + str(lr))\n        print(\"================================================\")\n      # argumentos para el entrenamiento\n        training_args = TrainingArguments(\n            #output_dir=\"my_checkpoint\"+str(j),\n            output_dir=f\"my_checkpoint_{i}\",\n            overwrite_output_dir = True,\n            num_train_epochs=10,\n            #para probar tanto promise como nuestro coger 10 epochs\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            save_total_limit = 1,\n            load_best_model_at_end=True,\n            logging_strategy='epoch',\n            optim=\"adamw_torch\",\n            per_device_train_batch_size=32,\n            #lo dejaremos en 32\n            #per_device_eval_batch_size=32,\n            learning_rate=lr,\n            #probar tanto 2e-5 como 5e-5\n            weight_decay=0.01,\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n        # declarar el \"entrenador\"\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_val_sets[j]['train'],\n            eval_dataset=train_val_sets[j]['test'],\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics\n        )\n\n        # realizar el entrenamiento\n        trainer.train()\n\n        out_pred = trainer.predict(data_train_val['test'])\n        if out_pred.metrics['test_f1'] > best_f1:\n          best_f1 = out_pred.metrics['test_f1']\n          best_lr = i\n      # cargar el mejor modelo y resultados para test\n      best_dir = glob.glob(f\"my_checkpoint_{best_lr}/checkpoint*\")\n      print(best_dir)\n      model_best = AutoModelForSequenceClassification.from_pretrained(best_dir[0])\n      trainer_best = Trainer(\n            model=model_best,\n            args=None,\n            train_dataset=None,\n            eval_dataset=None,\n            tokenizer=tokenizer,\n            compute_metrics=compute_metrics\n        )\n\n      test_data = test_sets[j]\n      outputs_pred = trainer_best.predict(test_dataset=test_data)\n\n      # model predictions\n      predictions = np.argmax(outputs_pred.predictions, axis=1)\n\n      #d_res = classification_report(test_data['labels'], predictions, digits=3, return_dict = True)\n      d_res = classification_report(test_data['labels'], predictions, digits=3, output_dict=True)\n      print(d_res)\n      f.write(\"================== Best lr ================\\n\")\n      f.write(str(best_lr))\n      f.write(\"\\n\")\n      f.write( \"================== Resultados ================\\n\")\n      f.write(str(d_res))","metadata":{"execution":{"iopub.status.busy":"2024-08-26T15:46:09.627703Z","iopub.execute_input":"2024-08-26T15:46:09.628385Z","iopub.status.idle":"2024-08-26T16:28:39.801897Z","shell.execute_reply.started":"2024-08-26T15:46:09.628335Z","shell.execute_reply":"2024-08-26T16:28:39.800960Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 0 // lr = 2e-05\n================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [430/430 02:40, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.376300</td>\n      <td>0.203839</td>\n      <td>0.645161</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.147700</td>\n      <td>0.135054</td>\n      <td>0.810811</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.080300</td>\n      <td>0.194660</td>\n      <td>0.764706</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.053900</td>\n      <td>0.222367</td>\n      <td>0.769231</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.037400</td>\n      <td>0.205316</td>\n      <td>0.810811</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.009800</td>\n      <td>0.288437</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.006000</td>\n      <td>0.275169</td>\n      <td>0.787879</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.008200</td>\n      <td>0.292314</td>\n      <td>0.787879</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.002800</td>\n      <td>0.274024</td>\n      <td>0.787879</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000400</td>\n      <td>0.280286</td>\n      <td>0.787879</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 0 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [430/430 02:42, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.304100</td>\n      <td>0.161044</td>\n      <td>0.722222</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.136000</td>\n      <td>0.134744</td>\n      <td>0.689655</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.080400</td>\n      <td>0.156016</td>\n      <td>0.838710</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.038700</td>\n      <td>0.155081</td>\n      <td>0.833333</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.010900</td>\n      <td>0.211391</td>\n      <td>0.848485</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.003900</td>\n      <td>0.243888</td>\n      <td>0.823529</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000300</td>\n      <td>0.270089</td>\n      <td>0.838710</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.001500</td>\n      <td>0.253764</td>\n      <td>0.838710</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.002700</td>\n      <td>0.251982</td>\n      <td>0.823529</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.256433</td>\n      <td>0.823529</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-86']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.8629441624365483, 'recall': 0.9784172661870504, 'f1-score': 0.9170600134861767, 'support': 695}, '1': {'precision': 0.8571428571428571, 'recall': 0.45454545454545453, 'f1-score': 0.594059405940594, 'support': 198}, 'accuracy': 0.8622620380739082, 'macro avg': {'precision': 0.8600435097897027, 'recall': 0.7164813603662524, 'f1-score': 0.7555597097133854, 'support': 893}, 'weighted avg': {'precision': 0.8616578707812841, 'recall': 0.8622620380739082, 'f1-score': 0.8454428575018259, 'support': 893}}\n================================================\nj = 1 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [630/630 03:44, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.403500</td>\n      <td>0.281231</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.200500</td>\n      <td>0.262576</td>\n      <td>0.690476</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.123900</td>\n      <td>0.251300</td>\n      <td>0.727273</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.090700</td>\n      <td>0.393930</td>\n      <td>0.705882</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.053400</td>\n      <td>0.503348</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.039600</td>\n      <td>0.493237</td>\n      <td>0.731707</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.015800</td>\n      <td>0.355175</td>\n      <td>0.794118</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.014100</td>\n      <td>0.381799</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.002300</td>\n      <td>0.481213</td>\n      <td>0.769231</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.009300</td>\n      <td>0.401168</td>\n      <td>0.800000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 1 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [630/630 03:43, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.367700</td>\n      <td>0.412183</td>\n      <td>0.243902</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.190300</td>\n      <td>0.255530</td>\n      <td>0.784810</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.126700</td>\n      <td>0.293210</td>\n      <td>0.734177</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.088000</td>\n      <td>0.438528</td>\n      <td>0.753247</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.036800</td>\n      <td>0.337914</td>\n      <td>0.845070</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.029000</td>\n      <td>0.366457</td>\n      <td>0.805556</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.005700</td>\n      <td>0.555654</td>\n      <td>0.779221</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.004400</td>\n      <td>0.544141</td>\n      <td>0.794872</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000200</td>\n      <td>0.575497</td>\n      <td>0.775000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003800</td>\n      <td>0.555580</td>\n      <td>0.789474</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-189']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.968944099378882, 'recall': 0.975, 'f1-score': 0.9719626168224299, 'support': 160}, '1': {'precision': 0.6666666666666666, 'recall': 0.6153846153846154, 'f1-score': 0.64, 'support': 13}, 'accuracy': 0.9479768786127167, 'macro avg': {'precision': 0.8178053830227743, 'recall': 0.7951923076923078, 'f1-score': 0.805981308411215, 'support': 173}, 'weighted avg': {'precision': 0.9462296102155364, 'recall': 0.9479768786127167, 'f1-score': 0.9470174490843282, 'support': 173}}\n================================================\nj = 2 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [650/650 03:54, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.408200</td>\n      <td>0.237986</td>\n      <td>0.693333</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.215200</td>\n      <td>0.173069</td>\n      <td>0.766667</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.138400</td>\n      <td>0.140381</td>\n      <td>0.818182</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.084100</td>\n      <td>0.148711</td>\n      <td>0.882353</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.044700</td>\n      <td>0.188711</td>\n      <td>0.891892</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.026900</td>\n      <td>0.190533</td>\n      <td>0.873239</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.009700</td>\n      <td>0.221712</td>\n      <td>0.876712</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.006400</td>\n      <td>0.248870</td>\n      <td>0.864865</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.001700</td>\n      <td>0.237163</td>\n      <td>0.876712</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003400</td>\n      <td>0.238855</td>\n      <td>0.876712</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 2 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [650/650 03:54, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.323700</td>\n      <td>0.207340</td>\n      <td>0.746269</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.191100</td>\n      <td>0.215258</td>\n      <td>0.761905</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.135800</td>\n      <td>0.224548</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.080200</td>\n      <td>0.216158</td>\n      <td>0.805970</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.035500</td>\n      <td>0.517192</td>\n      <td>0.678571</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.015600</td>\n      <td>0.209830</td>\n      <td>0.861111</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000500</td>\n      <td>0.237051</td>\n      <td>0.852941</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000200</td>\n      <td>0.317313</td>\n      <td>0.852941</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000300</td>\n      <td>0.342259</td>\n      <td>0.852941</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.347930</td>\n      <td>0.840580</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-195']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.9901960784313726, 'recall': 0.9351851851851852, 'f1-score': 0.961904761904762, 'support': 108}, '1': {'precision': 0.125, 'recall': 0.5, 'f1-score': 0.2, 'support': 2}, 'accuracy': 0.9272727272727272, 'macro avg': {'precision': 0.5575980392156863, 'recall': 0.7175925925925926, 'f1-score': 0.580952380952381, 'support': 110}, 'weighted avg': {'precision': 0.9744652406417113, 'recall': 0.9272727272727272, 'f1-score': 0.9480519480519483, 'support': 110}}\n================================================\nj = 3 // lr = 2e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [630/630 03:50, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.394700</td>\n      <td>0.347040</td>\n      <td>0.519481</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.199900</td>\n      <td>0.305487</td>\n      <td>0.657534</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.119800</td>\n      <td>0.341380</td>\n      <td>0.718750</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.079100</td>\n      <td>0.460899</td>\n      <td>0.677966</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.039700</td>\n      <td>0.559741</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.020100</td>\n      <td>0.506793</td>\n      <td>0.716418</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.017400</td>\n      <td>0.611713</td>\n      <td>0.695652</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.011300</td>\n      <td>0.610048</td>\n      <td>0.707692</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.005500</td>\n      <td>0.622654</td>\n      <td>0.696970</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003400</td>\n      <td>0.631887</td>\n      <td>0.696970</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 3 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [630/630 03:58, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.401600</td>\n      <td>0.302256</td>\n      <td>0.566667</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.201700</td>\n      <td>0.222642</td>\n      <td>0.705882</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.144000</td>\n      <td>0.383387</td>\n      <td>0.676471</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.078700</td>\n      <td>0.381959</td>\n      <td>0.696970</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.042200</td>\n      <td>0.508998</td>\n      <td>0.712329</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.022100</td>\n      <td>0.487395</td>\n      <td>0.746269</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.006400</td>\n      <td>0.550627</td>\n      <td>0.707692</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.008000</td>\n      <td>0.583652</td>\n      <td>0.707692</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000700</td>\n      <td>0.616300</td>\n      <td>0.707692</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000200</td>\n      <td>0.628268</td>\n      <td>0.707692</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-126']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.9473684210526315, 'recall': 0.9264705882352942, 'f1-score': 0.9368029739776952, 'support': 136}, '1': {'precision': 0.7297297297297297, 'recall': 0.7941176470588235, 'f1-score': 0.7605633802816901, 'support': 34}, 'accuracy': 0.9, 'macro avg': {'precision': 0.8385490753911806, 'recall': 0.8602941176470589, 'f1-score': 0.8486831771296927, 'support': 170}, 'weighted avg': {'precision': 0.9038406827880512, 'recall': 0.9, 'f1-score': 0.9015550552384941, 'support': 170}}\n================================================\nj = 4 // lr = 2e-05\n================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [470/470 03:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.441600</td>\n      <td>0.273846</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.257200</td>\n      <td>0.176150</td>\n      <td>0.807018</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.158800</td>\n      <td>0.248028</td>\n      <td>0.786885</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.094900</td>\n      <td>0.219150</td>\n      <td>0.842105</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.079500</td>\n      <td>0.192696</td>\n      <td>0.851852</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.048400</td>\n      <td>0.372755</td>\n      <td>0.757576</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.031900</td>\n      <td>0.360120</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.010800</td>\n      <td>0.363039</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.003800</td>\n      <td>0.325771</td>\n      <td>0.842105</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.002600</td>\n      <td>0.394166</td>\n      <td>0.800000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 4 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [470/470 03:04, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.438500</td>\n      <td>0.240319</td>\n      <td>0.742857</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.275200</td>\n      <td>0.222898</td>\n      <td>0.734694</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192700</td>\n      <td>0.211132</td>\n      <td>0.754098</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.124900</td>\n      <td>0.403814</td>\n      <td>0.695652</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.081000</td>\n      <td>0.247811</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.041800</td>\n      <td>0.365756</td>\n      <td>0.754098</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.020900</td>\n      <td>0.431967</td>\n      <td>0.806452</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.007900</td>\n      <td>0.406503</td>\n      <td>0.813559</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.003300</td>\n      <td>0.418260</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.006800</td>\n      <td>0.395034</td>\n      <td>0.779661</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_1/checkpoint-141']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"{'0': {'precision': 0.9698492462311558, 'recall': 0.9089481946624803, 'f1-score': 0.9384116693679092, 'support': 637}, '1': {'precision': 0.5503875968992248, 'recall': 0.797752808988764, 'f1-score': 0.6513761467889908, 'support': 89}, 'accuracy': 0.8953168044077136, 'macro avg': {'precision': 0.7601184215651903, 'recall': 0.8533505018256222, 'f1-score': 0.7948939080784501, 'support': 726}, 'weighted avg': {'precision': 0.918427639081649, 'recall': 0.8953168044077136, 'f1-score': 0.9032241190793089, 'support': 726}}\n================================================\nj = 5 // lr = 2e-05\n================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='590' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [590/590 03:40, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.435000</td>\n      <td>0.284911</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.241300</td>\n      <td>0.205205</td>\n      <td>0.794118</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.141200</td>\n      <td>0.213813</td>\n      <td>0.766667</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.092900</td>\n      <td>0.318337</td>\n      <td>0.756098</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.046600</td>\n      <td>0.220949</td>\n      <td>0.787879</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.047100</td>\n      <td>0.290592</td>\n      <td>0.849315</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.020500</td>\n      <td>0.267195</td>\n      <td>0.861111</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.013200</td>\n      <td>0.377708</td>\n      <td>0.815789</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.007700</td>\n      <td>0.294926</td>\n      <td>0.845070</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.005900</td>\n      <td>0.372050</td>\n      <td>0.837838</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"================================================\nj = 5 // lr = 5e-05\n================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='590' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [590/590 03:39, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.374700</td>\n      <td>0.272207</td>\n      <td>0.633333</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.233300</td>\n      <td>0.190932</td>\n      <td>0.776119</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.148000</td>\n      <td>0.239518</td>\n      <td>0.754098</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.130100</td>\n      <td>0.564406</td>\n      <td>0.601942</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.070600</td>\n      <td>0.240036</td>\n      <td>0.811594</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.045300</td>\n      <td>0.239640</td>\n      <td>0.833333</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.019100</td>\n      <td>0.241186</td>\n      <td>0.873239</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.007600</td>\n      <td>0.304903</td>\n      <td>0.837838</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.008000</td>\n      <td>0.277259</td>\n      <td>0.873239</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.003400</td>\n      <td>0.281986</td>\n      <td>0.873239</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"['my_checkpoint_0/checkpoint-118']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'0': {'precision': 0.9849056603773585, 'recall': 0.9125874125874126, 'f1-score': 0.9473684210526316, 'support': 286}, '1': {'precision': 0.5370370370370371, 'recall': 0.8787878787878788, 'f1-score': 0.6666666666666667, 'support': 33}, 'accuracy': 0.9090909090909091, 'macro avg': {'precision': 0.7609713487071978, 'recall': 0.8956876456876457, 'f1-score': 0.8070175438596492, 'support': 319}, 'weighted avg': {'precision': 0.9385744234800839, 'recall': 0.9090909090909091, 'f1-score': 0.9183303085299457, 'support': 319}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"AquÃ­ acaba el cÃ³digo de colab. Falta hacer el zip para poder descargar los resultados (y opcionalmente usar el bot para que me avise del final de la ejecuciÃ³n)","metadata":{}},{"cell_type":"code","source":"import requests\n\n# Definir la URL base y los parÃ¡metros\nurl = \"https://api.callmebot.com/whatsapp.php\"\nparams = {\n    \"phone\": \"+34653508040\",\n    \"apikey\": \"1512070\",\n    \"text\": \"ðŸ¤–Â¡EjecuciÃ³n terminada! ðŸ¤–\\nVe a echarle un vistazo â˜ðŸ¤“\"\n}\n\n# Enviar la solicitud GET\nresponse = requests.get(url, params=params)\n\n# Imprimir el estado de la respuesta\nprint(response.status_code)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:28:39.803300Z","iopub.execute_input":"2024-08-26T16:28:39.803636Z","iopub.status.idle":"2024-08-26T16:28:40.461022Z","shell.execute_reply.started":"2024-08-26T16:28:39.803599Z","shell.execute_reply":"2024-08-26T16:28:40.459984Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"200\n<p>Message to: +34653508040<p>Text to send: ðŸ¤–Â¡EjecuciÃ³n terminada! ðŸ¤–%0AVe a echarle un vistazo â˜ðŸ¤“<p><b>Message queued.</b> You will receive it in a few seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r results_temas_bert.zip results_temas_bert\nfrom IPython.display import FileLink \nFileLink(r'results_temas_bert.zip')","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:28:40.462221Z","iopub.execute_input":"2024-08-26T16:28:40.462565Z","iopub.status.idle":"2024-08-26T16:28:41.494195Z","shell.execute_reply.started":"2024-08-26T16:28:40.462529Z","shell.execute_reply":"2024-08-26T16:28:41.493123Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  adding: results_temas_roberta/ (stored 0%)\n  adding: results_temas_roberta/res_temas_roberta_0.txt (deflated 57%)\n  adding: results_temas_roberta/res_temas_roberta_1.txt (deflated 57%)\n  adding: results_temas_roberta/res_temas_roberta_4.txt (deflated 55%)\n  adding: results_temas_roberta/res_temas_roberta_3.txt (deflated 57%)\n  adding: results_temas_roberta/res_temas_roberta_5.txt (deflated 60%)\n  adding: results_temas_roberta/res_temas_roberta_2.txt (deflated 60%)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/results_temas_roberta.zip","text/html":"<a href='results_temas_roberta.zip' target='_blank'>results_temas_roberta.zip</a><br>"},"metadata":{}}]}]}